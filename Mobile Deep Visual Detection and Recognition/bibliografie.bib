
@book{jiang_deep_2019,
	address = {Singapore},
	edition = {1st ed. 2019.},
	title = {Deep {Learning} in {Object} {Detection} and {Recognition}, edited by {Xiaoyue} {Jiang}, {Abdenour} {Hadid}, {Yanwei} {Pang}, {Eric} {Granger}, {Xiaoyi} {Feng}.},
	isbn = {978-981-10-5152-4},
	abstract = {This book discusses recent advances in object detection and recognition using deep learning methods, which have achieved great success in the field of computer vision and image processing. It provides a systematic and methodical overview of the latest developments in deep learning theory and its applications to computer vision, illustrating them using key topics, including object detection, face analysis, 3D object recognition, and image retrieval. The book offers a rich blend of theory and practice. It is suitable for students, researchers and practitioners interested in deep learning, computer vision and beyond and can also be used as a reference book. The comprehensive comparison of various deep-learning applications helps readers with a basic understanding of machine learning and calculus grasp the theories and inspires applications in other computer vision tasks.},
	language = {eng},
	publisher = {Springer Singapore : Imprint: Springer},
	author = {Jiang, Xiaoyue and Hadid, Abdenour and Pang, Yanwei and Granger, Eric and Feng, Xiaoyi},
	year = {2019},
	keywords = {Data mining., Optical data processing., Pattern recognition.},
}

@misc{koehrsen_neural_2018,
	title = {Neural {Network} {Embeddings} {Explained}},
	url = {https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526},
	abstract = {How deep learning can represent War and Peace as a vector},
	language = {en},
	urldate = {2021-09-21},
	journal = {Medium},
	author = {Koehrsen, Will},
	month = oct,
	year = {2018},
	file = {Snapshot:files/56/neural-network-embeddings-explained-4d028e6f0526.html:text/html},
}

@misc{saha_comprehensive_2018,
	title = {A {Comprehensive} {Guide} to {Convolutional} {Neural} {Networks} — the {ELI5} way},
	url = {https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53},
	abstract = {Artificial Intelligence has been witnessing a monumental growth in bridging the gap between the capabilities of humans and machines…},
	language = {en},
	urldate = {2021-09-21},
	journal = {Medium},
	author = {Saha, Sumit},
	month = dec,
	year = {2018},
	file = {Snapshot:files/58/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53.html:text/html},
}

@article{du_overview_2020,
	title = {Overview of two-stage object detection algorithms},
	volume = {1544},
	issn = {1742-6596},
	url = {https://doi.org/10.1088/1742-6596/1544/1/012033},
	doi = {10.1088/1742-6596/1544/1/012033},
	abstract = {Nowadays, object detection has gradually become a quite popular field. From the traditional methods to the methods used at this stage, object detection technology has made great progress, and is still continuously developing and innovating. This paper reviews two-stage object detection algorithms used at this stage, explaining in detail the working principles of Faster R-CNN, R-FCN, FPN, and Casecade R-CNN and analyzing the similarities and differences between these four two-stage object detection algorithms. Then we used HSRC2016 ship dataset to perform experiments with Faster R-CNN, R-FCN, FPN, and Casecade R-CNN and compared the effectiveness of them with experimental results.},
	language = {en},
	urldate = {2021-09-21},
	journal = {Journal of Physics: Conference Series},
	author = {Du, Lixuan and Zhang, Rongyu and Wang, Xiaotian},
	month = may,
	year = {2020},
	note = {Publisher: IOP Publishing},
	pages = {012033},
	file = {IOP Full Text PDF:files/62/Du e.a. - 2020 - Overview of two-stage object detection algorithms.pdf:application/pdf},
}

@misc{brownlee_gentle_2019,
	title = {A {Gentle} {Introduction} to the {Rectified} {Linear} {Unit} ({ReLU})},
	url = {https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/},
	abstract = {In a neural network, the activation function is responsible for transforming the summed weighted input from the node into the […]},
	language = {en-US},
	urldate = {2021-09-21},
	journal = {Machine Learning Mastery},
	author = {Brownlee, Jason},
	month = jan,
	year = {2019},
}

@article{girshick_rich_2014,
	title = {Rich feature hierarchies for accurate object detection and semantic segmentation},
	url = {http://arxiv.org/abs/1311.2524},
	abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012---achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\textasciitilde}rbg/rcnn.},
	urldate = {2021-09-22},
	journal = {arXiv:1311.2524 [cs]},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	month = oct,
	year = {2014},
	note = {arXiv: 1311.2524},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Extended version of our CVPR 2014 paper; latest update (v5) includes results using deeper networks (see Appendix G. Changelog)},
	file = {arXiv Fulltext PDF:files/67/Girshick e.a. - 2014 - Rich feature hierarchies for accurate object detec.pdf:application/pdf;arXiv.org Snapshot:files/69/1311.html:text/html},
}

@article{uijlings_selective_2013,
	title = {Selective {Search} for {Object} {Recognition}},
	volume = {104},
	copyright = {Springer Science+Business Media New York 2013},
	issn = {09205691},
	url = {http://www.proquest.com/docview/1412093173/abstract/6D683BAB48F94F66PQ/1},
	doi = {http://dx.doi.org.kuleuven.e-bronnen.be/10.1007/s11263-013-0620-5},
	abstract = {This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 \% recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi.unitn.it/{\textasciitilde}uijlings/SelectiveSearch.html).[PUBLICATION ABSTRACT]},
	language = {English},
	number = {2},
	urldate = {2021-09-22},
	journal = {International Journal of Computer Vision},
	author = {Uijlings, J. R. and R and van de Sande, K. E. and A and Gevers, T. and Smeulders, A. W. and M},
	month = sep,
	year = {2013},
	note = {Num Pages: 154-171
Place: New York, Netherlands
Publisher: Springer Nature B.V.},
	keywords = {Computer science, Computers--Computer Systems, Datasets, Image processing systems, Information management, Localization, Search engines, Software, Studies},
	pages = {154--171},
	file = {Full Text PDF:files/68/Uijlings e.a. - 2013 - Selective Search for Object Recognition.pdf:application/pdf},
}

@article{ren_faster_2016,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	url = {http://arxiv.org/abs/1506.01497},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	urldate = {2021-09-22},
	journal = {arXiv:1506.01497 [cs]},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jan,
	year = {2016},
	note = {arXiv: 1506.01497},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Extended tech report},
	file = {arXiv Fulltext PDF:files/76/Ren e.a. - 2016 - Faster R-CNN Towards Real-Time Object Detection w.pdf:application/pdf;arXiv.org Snapshot:files/77/1506.html:text/html},
}

@misc{noauthor_ieee_nodate,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore-ieee-org.kuleuven.e-bronnen.be/stamp/stamp.jsp?tp=&arnumber=7780460},
	urldate = {2021-09-22},
	file = {IEEE Xplore Full-Text PDF\::files/79/stamp.html:text/html},
}

@inproceedings{redmon_you_2016,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	shorttitle = {You {Only} {Look} {Once}},
	doi = {10.1109/CVPR.2016.91},
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	month = jun,
	year = {2016},
	note = {ISSN: 1063-6919},
	keywords = {Computer architecture, Microprocessors, Neural networks, Object detection, Pipelines, Real-time systems, Training},
	pages = {779--788},
	file = {IEEE Xplore Full Text PDF:files/81/Redmon e.a. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf:application/pdf;IEEE Xplore Abstract Record:files/82/7780460.html:text/html},
}

@inproceedings{liu_ssd_2016,
	title = {{SSD}: {Single} {Shot} {MultiBox} {Detector}},
	volume = {9905},
	isbn = {978-3-319-46447-3},
	shorttitle = {{SSD}},
	doi = {10.1007/978-3-319-46448-0_2},
	abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For {\textbackslash}(300 {\textbackslash}times 300{\textbackslash}) input, SSD achieves 74.3 \% mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for {\textbackslash}(512 {\textbackslash}times 512{\textbackslash}) input, SSD achieves 76.9 \% mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https:// github. com/ weiliu89/ caffe/ tree/ ssd.},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander},
	month = oct,
	year = {2016},
	pages = {21--37},
	file = {Full Text PDF:files/89/Liu e.a. - 2016 - SSD Single Shot MultiBox Detector.pdf:application/pdf},
}

@book{leibe_computer_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Computer {Vision} – {ECCV} 2016: 14th {European} {Conference}, {Amsterdam}, {The} {Netherlands}, {October} 11–14, 2016, {Proceedings}, {Part} {I}},
	volume = {9905},
	isbn = {978-3-319-46447-3 978-3-319-46448-0},
	shorttitle = {Computer {Vision} – {ECCV} 2016},
	url = {http://link.springer.com/10.1007/978-3-319-46448-0},
	language = {en},
	urldate = {2021-09-24},
	publisher = {Springer International Publishing},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	doi = {10.1007/978-3-319-46448-0},
	file = {Leibe e.a. - 2016 - Computer Vision – ECCV 2016 14th European Confere.pdf:files/87/Leibe e.a. - 2016 - Computer Vision – ECCV 2016 14th European Confere.pdf:application/pdf},
}

@misc{little_activation_2020,
	title = {Activation {Functions} ({Linear}/{Non}-linear) in {Deep} {Learning} — {ReLU}/{Sigmoid}/{SoftMax}/{Swish}/{Leaky} {ReLu}.},
	url = {https://xzz201920.medium.com/activation-functions-linear-non-linear-in-deep-learning-relu-sigmoid-softmax-swish-leaky-relu-a6333be712ea},
	abstract = {What is the activation function?},
	language = {en},
	urldate = {2021-09-24},
	journal = {Medium},
	author = {Little, Z²},
	month = may,
	year = {2020},
	file = {Snapshot:files/93/activation-functions-linear-non-linear-in-deep-learning-relu-sigmoid-softmax-swish-leaky-relu-a.html:text/html},
}

@article{girshick_fast_2015,
	title = {Fast {R}-{CNN}},
	url = {http://arxiv.org/abs/1504.08083},
	abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
	urldate = {2021-09-24},
	journal = {arXiv:1504.08083 [cs]},
	author = {Girshick, Ross},
	month = sep,
	year = {2015},
	note = {arXiv: 1504.08083},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: To appear in ICCV 2015},
	file = {arXiv Fulltext PDF:files/99/Girshick - 2015 - Fast R-CNN.pdf:application/pdf;arXiv.org Snapshot:files/101/1504.html:text/html},
}

@article{han_deep_2016,
	title = {Deep {Compression}: {Compressing} {Deep} {Neural} {Networks} with {Pruning}, {Trained} {Quantization} and {Huffman} {Coding}},
	shorttitle = {Deep {Compression}},
	url = {http://arxiv.org/abs/1510.00149},
	abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.},
	urldate = {2021-09-26},
	journal = {arXiv:1510.00149 [cs]},
	author = {Han, Song and Mao, Huizi and Dally, William J.},
	month = feb,
	year = {2016},
	note = {arXiv: 1510.00149},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Published as a conference paper at ICLR 2016 (oral)},
	file = {arXiv Fulltext PDF:files/105/Han e.a. - 2016 - Deep Compression Compressing Deep Neural Networks.pdf:application/pdf;arXiv.org Snapshot:files/106/1510.html:text/html},
}

@misc{noauthor_PyTorch_nodate,
	title = {{PyTorch}},
	url = {https://www.PyTorch.org},
	abstract = {An open source machine learning framework that accelerates the path from research prototyping to production deployment.},
	language = {en},
	urldate = {2021-10-02},
	file = {Snapshot:files/108/home.html:text/html},
}

@misc{noauthor_tensorflow_nodate,
	title = {{TensorFlow}},
	url = {https://www.tensorflow.org/?hl=nl},
	abstract = {An end-to-end open source machine learning platform for everyone. Discover TensorFlow's flexible ecosystem of tools, libraries and community resources.},
	language = {en},
	urldate = {2021-10-02},
	journal = {TensorFlow},
	file = {Snapshot:files/110/www.tensorflow.org.html:text/html},
}

@misc{noauthor_onnx_nodate,
	title = {{ONNX} {\textbar} {Home}},
	url = {https://onnx.ai/},
	urldate = {2021-10-02},
	file = {ONNX | Home:files/112/onnx.ai.html:text/html},
}

@article{luo_comparison_2020,
	title = {Comparison and {Benchmarking} of {AI} {Models} and {Frameworks} on {Mobile} {Devices}},
	url = {http://arxiv.org/abs/2005.05085},
	abstract = {Due to increasing amounts of data and compute resources, deep learning achieves many successes in various domains. The application of deep learning on the mobile and embedded devices is taken more and more attentions, benchmarking and ranking the AI abilities of mobile and embedded devices becomes an urgent problem to be solved. Considering the model diversity and framework diversity, we propose a benchmark suite, AIoTBench, which focuses on the evaluation of the inference abilities of mobile and embedded devices. AIoTBench covers three typical heavy-weight networks: ResNet50, InceptionV3, DenseNet121, as well as three light-weight networks: SqueezeNet, MobileNetV2, MnasNet. Each network is implemented by three frameworks which are designed for mobile and embedded devices: Tensorflow Lite, Caffe2, PyTorch Mobile. To compare and rank the AI capabilities of the devices, we propose two unified metrics as the AI scores: Valid Images Per Second (VIPS) and Valid FLOPs Per Second (VOPS). Currently, we have compared and ranked 5 mobile devices using our benchmark. This list will be extended and updated soon after.},
	urldate = {2021-10-02},
	journal = {arXiv:2005.05085 [cs, eess]},
	author = {Luo, Chunjie and He, Xiwen and Zhan, Jianfeng and Wang, Lei and Gao, Wanling and Dai, Jiahui},
	month = may,
	year = {2020},
	note = {arXiv: 2005.05085},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Performance, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:files/114/Luo e.a. - 2020 - Comparison and Benchmarking of AI Models and Frame.pdf:application/pdf;arXiv.org Snapshot:files/115/2005.html:text/html},
}

@inproceedings{febvay_low-level_2020,
	address = {Seattle WA USA},
	title = {Low-level {Optimizations} for {Faster} {Mobile} {Deep} {Learning} {Inference} {Frameworks}},
	isbn = {978-1-4503-7988-5},
	url = {https://dl.acm.org/doi/10.1145/3394171.3416516},
	doi = {10.1145/3394171.3416516},
	abstract = {Over the last ten years, we have seen a strong progression of technology around smartphones. Each new generation acquires capabilities that significantly increase performance. On the other hand, several deep learning tools are offered today by the giants of the net for mobile, embedded devices and IoT. The proposed libraries allow a machine learning inference on the device with low latency. They provide pre-trained models, but one can also use one’s own models and run them on mobile, embedded or microcontroller devices. Lack of privacy, poor Internet connectivity and high cost of cloud platform let on-device inference became popular through app developers but there are more significant challenges especially for real-time tasks like augmented reality or autonomous driving. This PhD research aims at providing a path for developers to help them choose the best methods and tools to do real-time inference on mobile devices. In this paper, we present the performance benchmark of four popular open-source deep learning inference frameworks used on mobile devices on three different convolutional neural network models. We focus our work on image classification process and particularly on validation image bank of ImageNet 2012 dataset. We try to answer three questions : How does a framework influence model prediction and latency ? Why some frameworks are better in terms of latency/accuracy than others with the same model ? And what are the difficulties to implement these frameworks inside a mobile application ? Our first findings demonstrate that low-level software implementations chosen in frameworks, model conversion steps and parameters set in the framework have a big impact on performance and accuracy.},
	language = {en},
	urldate = {2021-10-02},
	booktitle = {Proceedings of the 28th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Febvay, Mathieu},
	month = oct,
	year = {2020},
	pages = {4738--4742},
	file = {Febvay - 2020 - Low-level Optimizations for Faster Mobile Deep Lea.pdf:files/117/Febvay - 2020 - Low-level Optimizations for Faster Mobile Deep Lea.pdf:application/pdf},
}

@misc{onnx_onnx_2017,
	title = {{ONNX} {Tutorials}},
	copyright = {Apache-2.0},
	author = {ONNX},
	year         = {2017},
	url = {https://github.com/onnx/tutorials},
	abstract = {Tutorials for creating and using ONNX models},
	urldate = {2021-10-02},
	publisher = {Open Neural Network Exchange},
	note = {original-date: 2017-11-15T18:59:18Z},
}

@misc{noauthor_api_nodate,
	title = {{API} {Documentation} {\textbar} {TensorFlow} {Core} v2.6.0},
	url = {https://www.tensorflow.org/api_docs?hl=nl},
	abstract = {An open source machine learning library for research and production.},
	language = {en},
	urldate = {2021-10-02},
	file = {Snapshot:files/124/api_docs.html:text/html},
}

@article{abadi_tensorflow_2016,
	title = {{TensorFlow}: {A} system for large-scale machine learning},
	shorttitle = {{TensorFlow}},
	url = {http://arxiv.org/abs/1605.08695},
	abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
	urldate = {2021-10-02},
	journal = {arXiv:1605.08695 [cs]},
	author = {Abadi, Martín and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	month = may,
	year = {2016},
	note = {arXiv: 1605.08695},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing},
	annote = {Comment: 18 pages, 9 figures; v2 has a spelling correction in the metadata},
	file = {arXiv Fulltext PDF:files/127/Abadi e.a. - 2016 - TensorFlow A system for large-scale machine learn.pdf:application/pdf;arXiv.org Snapshot:files/128/1605.html:text/html},
}

@article{li_PyTorch_2020,
	title = {{PyTorch} distributed: experiences on accelerating data parallel training},
	volume = {13},
	issn = {2150-8097},
	shorttitle = {{PyTorch} distributed},
	url = {https://dl.acm.org/doi/10.14778/3415478.3415530},
	doi = {10.14778/3415478.3415530},
	abstract = {This paper presents the design, implementation, and evaluation of the PyTorch distributed data parallel module. PyTorch is a widely-adopted scientiﬁc computing package used in deep learning research and applications. Recent advances in deep learning argue for the value of large datasets and large models, which necessitates the ability to scale out model training to more computational resources. Data parallelism has emerged as a popular solution for distributed training thanks to its straightforward principle and broad applicability. In general, the technique of distributed data parallelism replicates the model on every computational resource to generate gradients independently and then communicates those gradients at each iteration to keep model replicas consistent. Despite the conceptual simplicity of the technique, the subtle dependencies between computation and communication make it non-trivial to optimize the distributed training eﬃciency. As of v1.5, PyTorch natively provides several techniques to accelerate distributed data parallel, including bucketing gradients, overlapping computation with communication, and skipping gradient synchronization. Evaluations show that, when conﬁgured appropriately, the PyTorch distributed data parallel module attains near-linear scalability using 256 GPUs.},
	language = {en},
	number = {12},
	urldate = {2021-10-02},
	journal = {Proceedings of the VLDB Endowment},
	author = {Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and Chintala, Soumith},
	month = aug,
	year = {2020},
	pages = {3005--3018},
	file = {Li e.a. - 2020 - PyTorch distributed experiences on accelerating d.pdf:files/129/Li e.a. - 2020 - PyTorch distributed experiences on accelerating d.pdf:application/pdf},
}
