{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThijsVercammen/Masterproef/blob/main/Faster_RCNN_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlkPavTAdmC3"
      },
      "source": [
        "Instaleer nodige tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMFc2q3YNPub"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "\n",
        "pip install cython\n",
        "pip install torchvision\n",
        "!pip install \"opencv-python-headless<4.3\"\n",
        "!pip install brambox\n",
        "pip install onnxruntime\n",
        "wget https://machinelearningmastery.com/wp-content/uploads/2019/03/zebra.jpg\n",
        "\n",
        "git clone https://github.com/pdollar/coco/\n",
        "cd coco/PythonAPI\n",
        "make\n",
        "python setup.py install\n",
        "cd ../..\n",
        "rm -r coco"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvWRATEbdvZ6"
      },
      "source": [
        "Importeer de nodige bibliotheken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_roEM9CcMlR"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "from torchvision.io import read_image\n",
        "import torchvision.transforms.functional as F\n",
        "from collections import OrderedDict\n",
        "from typing import Tuple, List, Dict, Optional, Union\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
        "from torch import nn, Tensor\n",
        "import torch.onnx\n",
        "import onnxruntime as ort\n",
        "import datetime\n",
        "from datetime import datetime\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESbbtCH1d31_"
      },
      "source": [
        "Imorteer het Faster-RCNN model en voer het uit een test afbeelding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuX0H6xvl4hY"
      },
      "outputs": [],
      "source": [
        "m = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "im = Image.open(\"/content/zebra.jpg\")\n",
        "res_im = im.resize((224, 224))\n",
        "convert_tensor = torchvision.transforms.ToTensor()\n",
        "a = convert_tensor(res_im)\n",
        "img2 = F.convert_image_dtype(a, torch.float)\n",
        "img1 = F.convert_image_dtype(a, torch.uint8)\n",
        "\n",
        "m.eval()\n",
        "\n",
        "now = datetime.now()\n",
        "with torch.no_grad():\n",
        "    prediction = m([img2])\n",
        "print(\"Snelheid: {}\".format(datetime.now()-now))\n",
        "\n",
        "boxes = prediction[0]['boxes']\n",
        "\n",
        "print(prediction)\n",
        "drawn_boxes = draw_bounding_boxes(img1, boxes, colors=\"red\")\n",
        "Image.fromarray(drawn_boxes.mul(255).permute(1, 2, 0).byte().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dj_pjv-eBhd"
      },
      "source": [
        "Splits het Faster-RCNN model op in verschillende componenten en voer elke component apart uit op de testafbeelding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Od1HTs-pkbAq"
      },
      "outputs": [],
      "source": [
        "m.cpu()\n",
        "transf = m.transform\n",
        "backbone = m.backbone\n",
        "rpn = m.rpn\n",
        "roi_heads = m.roi_heads\n",
        "#rpn_a = m.rpn.anchor_generator\n",
        "#rpn_h = m.rpn.head\n",
        "#roi_heads_1 = m.roi_heads.box_roi_pool\n",
        "#roi_heads_2 = m.roi_heads.box_head\n",
        "#roi_heads_3 = m.roi_heads.box_predictor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wco4w8t4X7Y_"
      },
      "outputs": [],
      "source": [
        "img1 = read_image(\"/content/zebra.jpg\")\n",
        "img = F.convert_image_dtype(img1, torch.float)\n",
        "original_image_sizes: List[Tuple[int, int]] = []\n",
        "\n",
        "val = img.shape[-2:]\n",
        "assert len(val) == 2\n",
        "original_image_sizes.append((val[0], val[1]))\n",
        "\n",
        "#transform input\n",
        "imgages, targets = transf([img])\n",
        "\n",
        "#extract features\n",
        "features  = backbone(imgages.tensors)\n",
        "\n",
        "#region proposals\n",
        "prop, prop_l = rpn(imgages, features)\n",
        "\n",
        "#detections\n",
        "det, det_l = roi_heads(features, prop, imgages.image_sizes, targets)\n",
        "det_p = transf.postprocess(det, imgages.image_sizes, original_image_sizes)\n",
        "\n",
        "boxes = det_p[0]['boxes']\n",
        "\n",
        "print(det_p)\n",
        "drawn_boxes = draw_bounding_boxes(img1, boxes, colors=\"red\")\n",
        "Image.fromarray(drawn_boxes.mul(255).permute(1, 2, 0).byte().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twp0QjwMeS-A"
      },
      "source": [
        "Sla het model op en converteer het naar een model dat kan uitgevoerd worden in android studio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYAy6sT7YdWt"
      },
      "outputs": [],
      "source": [
        "m = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "torch.save(m, \"model.pth\")\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "example = torch.randn(1, 3, 160,160)\n",
        "m.eval()\n",
        "m.to('cpu')\n",
        "\n",
        "traced_script_module = torch.jit.script(m)\n",
        "traced_script_module.to('cpu')\n",
        "traced_script_module.save('./model.pt')\n",
        "\n",
        "#transf.to('cpu')\n",
        "#scripted_transf = torch.jit.script(transf)\n",
        "#scripted_transf.save('./transf.pt')\n",
        "\n",
        "#backbone.to('cpu')\n",
        "#traced_backbone = torch.jit.script(backbone)\n",
        "#traced_backbone.save('./backbone.pt')\n",
        "\n",
        "#rpn_h.to('cpu')\n",
        "#scripted_rpn_h = torch.jit.script(rpn_h)\n",
        "#scripted_rpn.save('./rpn_anchors.pt')\n",
        "\n",
        "# optimize posible for mobile\n",
        "#rpn.to('cpu')\n",
        "#scripted_rpn = torch.jit.script(rpn)\n",
        "#scripted_rpn.save('./rpn.pt')\n",
        "\n",
        "#roi_heads_1.to('cpu')\n",
        "#scripted_roi_heads_1 = torch.jit.script(roi_heads_1)\n",
        "#scripted_roi_heads_1_opt = optimize_for_mobile(scripted_roi_heads_1)\n",
        "#scripted_roi_heads_1_opt.save('./roi_h_1.pt')\n",
        "\n",
        "\n",
        "#roi_heads_2.to('cpu')\n",
        "#scripted_roi_heads_2 = torch.jit.script(roi_heads_2)\n",
        "#scripted_roi_heads_2_opt = optimize_for_mobile(scripted_roi_heads_2)\n",
        "#scripted_roi_heads_2_opt._save_for_lite_interpreter(\"mod1.ptl\")\n",
        "#scripted_roi_heads_2.save('./roi_h_2.pt')\n",
        "\n",
        "\n",
        "#roi_heads_3.to('cpu')\n",
        "#scripted_roi_heads_3 = torch.jit.script(roi_heads_3)\n",
        "#scripted_roi_heads_3_opt = optimize_for_mobile(scripted_roi_heads_3)\n",
        "#scripted_roi_heads_3_opt._save_for_lite_interpreter(\"mod1.ptl\")\n",
        "#scripted_roi_heads_3.save('./roi_h_3.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wXda-7iegzV"
      },
      "source": [
        "Voer het geconverteerde model uit op de testafbeelding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXmqydfWN_7c"
      },
      "outputs": [],
      "source": [
        "im = Image.open(\"/content/zebra.jpg\")\n",
        "res_im = im.resize((416, 416))\n",
        "convert_tensor = transforms.ToTensor()\n",
        "a = convert_tensor(res_im)\n",
        "\n",
        "now = datetime.now()\n",
        "out = traced_script_module([a])\n",
        "print(\"Snelheid: {}\".format(datetime.now()-now))\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkZQSOIMeqCK"
      },
      "source": [
        "Converteer het model naar ONNX formaat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrpBOcNg1sB1"
      },
      "outputs": [],
      "source": [
        "example = torch.randn(1, 3, 416,416)\n",
        "example.to('cpu')\n",
        "torch.onnx.export(m,               # model being run\n",
        "                  example,                         # model input (or a tuple for multiple inputs)\n",
        "                  \"model_py.onnx\",   # where to save the model (can be a file or file-like object)\n",
        "                  export_params=True,        # store the trained parameter weights inside the model file\n",
        "                  opset_version=12,         # the ONNX version to export the model to\n",
        "                  do_constant_folding=True,\n",
        "                  input_names = ['input_1'],   # the model's input names\n",
        "                  output_names = ['output'],\n",
        "                  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dREQIlZeu5O"
      },
      "source": [
        "Voer het ONNX model uit met een testafbeelding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EANhzvnkdq-8"
      },
      "outputs": [],
      "source": [
        "im = Image.open(\"/content/zebra.jpg\")\n",
        "res_im = im.resize((416, 416))\n",
        "convert_tensor = transforms.ToTensor()\n",
        "\n",
        "a = convert_tensor(res_im)\n",
        "img = F.convert_image_dtype(a, torch.float)\n",
        "b = img.unsqueeze(0)\n",
        "input_data = np.asarray(img, dtype=np.float32)\n",
        "\n",
        "\n",
        "ort_session = ort.InferenceSession(\"model_py.onnx\")\n",
        "\n",
        "now = datetime.now()\n",
        "outputs = ort_session.run(\n",
        "    None,\n",
        "    {\"input_1\": [input_data]},\n",
        ")\n",
        "print(\"Snelheid: {}\".format(datetime.now()-now))\n",
        "outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxlhArUae3nQ"
      },
      "source": [
        "Download testdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zokp5bjoPA8-",
        "outputId": "aba2ebb8-82b3-4c8c-ec6b-82287b4e682c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.78s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "!wget http://images.cocodataset.org/zips/val2017.zip\n",
        "!unzip val2017.zip\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "!unzip annotations_trainval2017.zip\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "\n",
        "path2data=\"/content/val2017\"\n",
        "path2json=\"/content/annotations/instances_val2017.json\"\n",
        "\n",
        "coco_train = dset.CocoDetection(root = path2data,\n",
        "                                annFile = path2json)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfCNk5MifMOa"
      },
      "source": [
        "Evalueer het standaard, mobiel en ONNX model\n",
        "(coco.names file met de coco labels moet hiervoor geupload worden)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGvrA2Z0RB9e"
      },
      "outputs": [],
      "source": [
        "from torch.nn.modules.module import T\n",
        "\n",
        "ort_session = ort.InferenceSession(\"model_py.onnx\")\n",
        "\n",
        "cc = coco_train.coco\n",
        "m.eval()\n",
        "z = 0\n",
        "class_label_map=[lines.strip() for lines in open('coco.names')]\n",
        "results_cs = []\n",
        "results_cs_m = []\n",
        "results_cs_o = []\n",
        "\n",
        "for x in cc.imgs:\n",
        "  val = cc.imgs.get(x)\n",
        "  imgpath = \"/content/val2017/\"+val['file_name']\n",
        "  print(\"index: {} Image: {}\".format(z, imgpath))\n",
        "  im = Image.open(imgpath)\n",
        "  resized_scale = 416\n",
        "\n",
        "  res_im = im.resize((resized_scale, resized_scale))\n",
        "  convert_tensor = torchvision.transforms.ToTensor()\n",
        "  tensor = convert_tensor(res_im)\n",
        "  img1 = F.convert_image_dtype(tensor, torch.float)\n",
        "  input_data = np.asarray(img1, dtype=np.float32)\n",
        "\n",
        "  width = val['width']\n",
        "  height = val['height']\n",
        "\n",
        "  imageName = val['file_name']\n",
        "  image_e = imageName[:12]\n",
        "\n",
        " \n",
        "  try:\n",
        "    with torch.no_grad():\n",
        "      prediction_1 = m([img1])\n",
        "      prediction_2 = traced_script_module([img1])\n",
        "\n",
        "\n",
        "    scores = prediction_1[0]['scores']\n",
        "    labels = prediction_1[0]['labels']\n",
        "    boxes = prediction_1[0]['boxes']\n",
        "\n",
        "    \n",
        "    for i in range(len(scores)):\n",
        "      if float(scores[i]) >= 0.5:\n",
        "        box1 = [float((boxes[i][0]/resized_scale)*width), float((boxes[i][1]/resized_scale)*height), float((boxes[i][2]/resized_scale)*width), float((boxes[i][3]/resized_scale)*height)]\n",
        "        w1, h1 = box1[2]-box1[0], box1[3]-box1[1]\n",
        "        results_cs.append([int(image_e), class_label_map[int(labels[i])-1], None, box1[0], box1[1], w1, h1, float(scores[i])])\n",
        "  except:\n",
        "    print(\"error default model {} \".format(val))\n",
        "\n",
        "\n",
        "  try:\n",
        "    scores = prediction_2[1][0]['scores']\n",
        "    labels = prediction_2[1][0]['labels']\n",
        "    boxes = prediction_2[1][0]['boxes']\n",
        "\n",
        "    for i in range(len(scores)):\n",
        "      if float(scores[i]) >= 0.5:\n",
        "        box1 = [float((boxes[i][0]/resized_scale)*width), float((boxes[i][1]/resized_scale)*height), float((boxes[i][2]/resized_scale)*width), float((boxes[i][3]/resized_scale)*height)]\n",
        "        w1, h1 = box1[2]-box1[0], box1[3]-box1[1]\n",
        "        results_cs_m.append([int(image_e), class_label_map[int(labels[i])-1], None, box1[0], box1[1], w1, h1, float(scores[i])])\n",
        "  except:\n",
        "    print(\"error mobiel model {} \".format(val))\n",
        "\n",
        "  try:\n",
        "    outputs = ort_session.run(None, {\"input_1\": [input_data]})\n",
        "    scores = outputs[2]\n",
        "    labels = outputs[1]\n",
        "    boxes = outputs[0]\n",
        "\n",
        "    for i in range(len(scores)):\n",
        "      if float(scores[i]) >= 0.5:\n",
        "        box1 = [float((boxes[i][0]/resized_scale)*width), float((boxes[i][1]/resized_scale)*height), float((boxes[i][2]/resized_scale)*width), float((boxes[i][3]/resized_scale)*height)]\n",
        "        w1, h1 = box1[2]-box1[0], box1[3]-box1[1]\n",
        "        results_cs_o.append([int(image_e), class_label_map[int(labels[i])-1], None, box1[0], box1[1], w1, h1, float(scores[i])])\n",
        "  except:\n",
        "    print(\"error onnx model {} \".format(val))\n",
        "\n",
        "  z += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCIosmQmfaM_"
      },
      "source": [
        "Evalueer resultaat met de brambox bibliotheek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVRferMtQFV1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Basic imports\n",
        "import brambox as bb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Settings\n",
        "sns.set(style='darkgrid', context='notebook')  # Nicer plotting colors\n",
        "bb.logger.setConsoleLevel('ERROR')             # Only show error log messages\n",
        "\n",
        "anno = bb.io.load('anno_coco', '/content/annotations/instances_val2017.json')\n",
        "anno.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "li7ERZhYNzrh"
      },
      "outputs": [],
      "source": [
        "fields = ['image', 'class_label', 'id', 'x_top_left', 'y_top_left', 'width', 'height', \"confidence\"]\n",
        "df = pd.DataFrame(results_cs_o, columns =fields)\n",
        "df.to_pickle(\"res.pkl\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljl-NA1BTDAm"
      },
      "outputs": [],
      "source": [
        "det = bb.io.load(\n",
        "    'pandas',\n",
        "    'res.pkl',\n",
        "    class_label_map\n",
        "    # We need to map the numbers from the JSON file to actual classnames\n",
        "    #class_label_map=[lines.strip() for lines in open('coco.names')]\n",
        ")\n",
        "\n",
        "# The image labels are just numbers in the COCO json files, so we need to rename those as well\n",
        "det.image = det.image.cat.rename_categories(lambda img: f'{img:012}')\n",
        "\n",
        "# Finally, not all images might contain detections,\n",
        "# and because the coco detection format does not contain a list of all images, we need to fix this ourselves\n",
        "# Note that this is optional, but you will get a warning if you dont do this\n",
        "det.image = det.image.cat.add_categories(set(anno.image.cat.categories) - set(det.image.cat.categories))\n",
        "\n",
        "det.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ox__1WO6TMYJ"
      },
      "outputs": [],
      "source": [
        "# Filter detections\n",
        "det_filtered = det.sort_values('confidence').groupby('image').head(100)\n",
        "\n",
        "# For each class\n",
        "aps = []\n",
        "for label in anno.class_label.unique():\n",
        "    ac = anno[anno.class_label == label]\n",
        "    dc = det_filtered[det_filtered.class_label == label]\n",
        "\n",
        "    # Compute smoothed PR\n",
        "    pr = bb.stat.pr(dc, ac, 0.5, smooth=True)\n",
        "\n",
        "    # Compute interpolated Riemann\n",
        "    aps.append(bb.stat.auc_interpolated(pr))\n",
        "\n",
        "# Compute average mAP\n",
        "mAP_50 = sum(aps) / len(aps)\n",
        "\n",
        "mAP_50"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "coco = bb.eval.COCO(det, anno)\n",
        "coco.mAP"
      ],
      "metadata": {
        "id": "UgNZ_XOEGESK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Faster_RCNN_PyTorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPGs8Q/QrjqLu43zSQ6ASn6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}