\chapter{Compatibiliteit van detectie systemen}
Voor detectiesystemen bestuderen we uitgebreid de mobiele implementatie van de Faster-RCNN architectuur met een ResNet50 backbone en de YOLO architectuur.
Voor deze modellen vertrekken we vanuit het PyTorch en TensorFlow framework om vervolgens de mogelijke paden te bestuderen naar een mobiele implementatie.
Hierbij maken we gebruik van Google Colaboratory met een CPU runtime om de modellen in te laden en te converteren.
Om het geconverteerde model te implementeren op een mobiel toestel gebruiken we Android Studio.
De volledige ge\"implementeerde code is terug te vinden in de volgende github repository: \url{https://github.com/ThijsVercammen/Masterproef.git}.

\section{Faster-RCNN naar mobiel mobiele implementatie}
Het Faster-RCNN model waarmee we starten is terug te vinden in de TensorFlow object detection API.
Dit Faster-RCNN model is voorgetraind met de COCO 2017 dataset (\cite{lin2015microsoft}) en maakt gebruik van een ResNet50 backbone.
We gaan dit model converteren naar een ONNX en TFLite formaat zodat dit model ge\"implementeerd kan worden in Android studio.
Vervolgens vertrekken we vanuit PyTorch waar we het Faster-RCNN model kunnen terugvinden in de Torchvision bibliotheek.
Dit model is ook een Faster-RCNN model dat voorgetraind is op de COCO 2017 dataset.
Vervolgens converteren we het Torchvision model naar een ONNX of PyTorch mobile formaat dat we kunnen implementeren in Android studio.

\subsection{Van TensorFlow naar TFLite implementatie} \label{rcnn_tf}
Het Faster-RCNN model van de TensorFlow object detection API is terug te vinden in de TensorFlow Hub.
Dit model kan eenvoudig worden ingeladen met de volgende Python code.

\begin{python}
import tensorflow_hub as hub
hub_model = hub.load("https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1")
\end{python}

De TensorFlow object detection API stelt zelf een manier voor om een model te converteren naar het TFLite formaat.
Hierbij wordt via een script (export\_tflite\_graph\_tf2.py) een model gegenereerd dat geoptimaliseerd is voor TFLite conversie.
Op deze manier zou de rest van de conversie gelijkaardig moeten zijn aan de conversie van het ResNet50 netwerk besproken in \ref{tf_h_conv}.
Maar de TensorFlow object detection API ondersteund enkel de TFLite conversie voor de SSD en Centernet architecturen.
Als we het script toch proberen uit te voeren dan krijgen we de volgende error: 
\textcolor{red}{ValueError: Only ssd or center\_net models are supported in tflite. Found faster\_rcnn in config.}

Als we het model willen converteren zonder gebruik te maken van het optimalisatie script zullen we standaard TensorFlow operaties aan het TFLite model moeten toevoegen.
Deze operaties moeten worden toegevoegd omdat we de ConcatV2 operatie niet kunnen converteren naar de TFLite concatenation operatie.
% waarom?????????????????????????????????????????
Om dit te realiseren moet de TensorFlow core bibliotheek worden toegevoegd aan Android studio zodat alle operaties uitgevoerd kunnen worden.

\begin{python}
converter = tf.lite.TFLiteConverter.from_keras_model(hub_model) # init converter
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
    tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
tflite_model = converter.convert() # converteer
open('model.tflite', 'wb').write(tflite_model) # model opslaan
\end{python}

Na het uitvoeren van bovenstaande Python code hebben we een TFLite model dat een input verwacht van de vorm [1,1,1,3].
Om dit TFLite model te kunnen uitvoeren moeten we de grootte van de inputafbeelding naar een hoogte en een breedte van 1 veranderen.
Een input die bestaat uit 1 pixel zou nooit voldoende informatie bevatten om objecten in de originele afbeelding te gaan detecteren.

Om dit probleem op te lossen defini\"eren we het model van de TensorFlow object detection API als een Keras laag.
Op deze manier kunnen we de input specifi\"eren en extra lagen gaan toevoegen via onderstaande Python code.

\begin{python} \label{testref}
layer = hub.KerasLayer(hub_model) # definieer als Keras laag
inputs = tf.keras.Input(shape=[416,416,3], dtype=tf.uint8) # specifieer input
x = layer(x) # genereer een output
output = [x["detection_classes"], x["detection_boxes"], x["detection_scores"], x["num_detections"]]
model = tf.keras.Model(inputs, output) # groepeer lagen tot model
\end{python}

Het formaat van de input kunnen we kiezen.
Een groot formaat geeft een beter resultaat, maar bevat meer data dus er moeten meer berekeningen worden uitgevoerd.
Een klein formaat geeft een minder goed resultaat, maar is sneller omdat er minder berekeningen uitgevoerd moeten worden.
Het datatype van de input moet uint8 zijn omdat het ingeladen model dit datatype verwacht.
Een ander voordeel van dit model is dat ConcatV2 operaties zonder problemen kunnen omgezet worden in de TFLite concatenation operatie.

De TFLiteConverter zal de namen van de verschillende outputs van het Faster-RCNN model veranderen.
Tijdens de conversie zal de volgorde van de vier outputs willekeurig veranderd worden.
De 4 outputs die wij zullen gebruiken in de Android studio implementatie zijn:

\begin{itemize}
	\item detection\_classes \textrightarrow StatefulPartitionedCall:0
	\item detection\_boxes \textrightarrow StatefulPartitionedCall:1
	\item detection\_scores \textrightarrow StatefulPartitionedCall:2
    \item num\_detections \textrightarrow StatefulPartitionedCall:3
\end{itemize}

%Voor een object detectie model te converteren kan er ook gebruik gemaakt worden van de standaard TFLiteConverter.
%Bij het converteren naar TFLite kan de ConcatV2 opperatie niet geconverteerd worden naar de TFLite concatenation opperatie.
%in het huidig model zijn er 2 gevallen waarbij de tf.ConcatV2 niet wordt geconverteerd naar tfl.concatenation.
%wat deze 2 gevallen van tf.ConcatV2 gemeenschappelijk hebben is dat zij een input krijgen van de tfl.div opperatie die is geconverteerd van tf.Realdiv.
%Door het toevoegen van TensorFlow Flex opperaties kan het model zonder problemen geconverteerd worden naar een TFLite model.
%Omdat we gebruik maken van Flex opperaties moet de TensorFlow core bibliotheek mee ge\"implementeerd worden in Android studio.
In tabel \ref{tab:TF_det_op} zien we wat er gebeurt met de operaties van het Faster-RCNN model tijdens de conversie naar TFLite.
Uiteraard komen de operaties van het ResNet50 model eerder besproken in (\ref{tab:TFop}) ook voor in het Faster-RCNN.
Deze operaties zullen op dezelfde manier geconverteerd worden naar TFLite.

Om Android studio zelf de code te laten genereren zoals bij het herkenningssysteem moeten we Metadata aan het model toevoegen. 
Bij het toevoegen van Metadata aan het TFLite model krijgen we de volgende fout: \textcolor{red}{Keyerror 2708.}
Deze fout geeft weinig informatie, maar de oorzaak is dat de methode die de Metadata aan het model toevoegt maximaal 4 outputs verwacht.
Het geconverteerd Faster-RCNN model heeft echter 8 outputs.
Door het aantal outputs te reduceren tot 4 outputs kunnen we Metadata succesvol aan het model toevoegen. % \ref{testref}.
Het uitvoeren van het model met Metadata in Android studio geeft de volgende error: 
\textcolor{red}{java.lang.IllegalArgumentException: Cannot copy from a TensorFlowLite tensor (StatefulPartitionedCall:2) with 1200 bytes to a Java Buffer with 4 bytes.}
\newline
Deze fout onstaat doordat tijdens het converteren naar TFLite de output informatie is gewijzigd.
De converter wijzigt namelijk de grootte van de output arrays naar 1, terwijl er meerdere resultaten worden geproduceerd.
Bijvoorbeeld de output van de detection\_boxes is [1, 300, 4], maar volgens de metadata is de output grootte [1, 1, 1] voor de bounding box co\"ordinaten.
Android studio genereert volgens de metadata een output buffer met grootte [1, 1, 1] die veel te klein is.

Om de grootte van de output buffers zelf te defini\"eren kunnen we gebruik maken van de TensorFlow Lite Interpreter API.
Via onderstaande Java code kunnen we het TFLite model uitvoeren in Android studio door gebruik te maken van de TensorFlow Lite Interpreter API

\begin{python}
Interpreter.Options tfliteOptions = new Interpreter.Options();
Interpreter tflite = new Interpreter(loadModelFile(), tfliteOptions);
tflite.runForMultipleInputsOutputs(inputs, outputs);

private MappedByteBuffer loadModelFile() throws IOException {
    AssetFileDescriptor fileDescriptor = this.getAssets().openFd("model.tflite");
    FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());
    FileChannel fileChannel = inputStream.getChannel();
    long startOffset = fileDescriptor.getStartOffset();
    long declaredLength = fileDescriptor.getDeclaredLength();
    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);
}
\end{python}

Hierbij kunnen we het TFLite model implementeren zonder er metadata aan toe te voegen.
De vereiste informatie om de correcte buffers te defini\"eren kan uit het niet geconverteerde model gehaald worden.
Het TFLite model bevat de juiste informatie voor de inputbuffer.
via deze informatie kan de juiste inputbuffer worden aangemaakt.
Voor de outputbuffers geeft de TensorFlow Lite Inerpreter API ons de mogelijkheid om de outputbuffers aan te passen zodat deze de gewenste grootte hebben.
Op deze manier kunnen we succesvol een Faster-RCNN model uitvoeren op een mobiel apparaat.
De onderstaande Java code definieert de outputbuffer voor de bounding boxen in Android studio.

\begin{python}
if(tflite.getOutputTensor(i).equals("StatefulPartitionedCall:1")) {
    int[] shape = tflite.getOutputTensor(i).shape();
    shape[1] = 300;
    shape[2] = 4;
    float[][][] boxesBuffer = new float[1][300][4];
    outputs.put(i, boxesBuffer);
}
\end{python}

Als we al de outputbuffers hebben aangemaakt kan het model worden uitgevoerd.
Vervolgens kunnen we dan alle bounding boxen bepalen waarvan de scores boven een bepaalde grens liggen.

\subsection{Van TensorFlow naar ONNX implementatie}
Het TensorFlow Faster-RCNN model kunnen we op de zelfde manier converteren naar ONNX als het ResNet50 model eerder beschreven in \ref{classonnx}.
Wel moeten we tijdens de conversie naar ONNX gebruik maken van opset versie 11 of hoger.
In het Faster-RCNN model wordt er namelijk gebruik gemaakt van de NonMaxSuppressionV5 operatie die pas beschikbaar is vanaf opset versie 11.
Al de andere operaties van het Faster-RCNN model worden ondersteund in eerdere opset versies.
In tabel \ref{tab:TF_det_op} zien we vanaf welke opset versie elke operatie wordt ondersteund.

Het gegenereerde ONNX model kunnen we op dezelfde manier als het ResNet50 model implementeren in Android studio.
Het TensorFlow Faster-RCNN model verwacht echter een input van het type Uint8.
Tijdens de conversie blijft het type input hetzelfde, maar de Onnxruntime API voor Android studio ondersteunt het Uint8 datatype niet.
Daarvoor moeten we eerst met onderstaande Python code een cast operatie moeten toevoegen aan het model.
Deze cast operatie zet een Float32 datatype om naar een Uint8 datatype.

\begin{python}
layer = hub.KerasLayer(hub_model) # definieer als Keras laag
inputs = tf.keras.Input(shape=[160,160,3], dtype=tf.float32) # specifieer input
x = tf.cast(inputs, dtype=tf.uint8) # cast input naar gewenste formaat
x = layer(x) # genereer een output
output = [x["detection_classes"], x["detection_boxes"], x["detection_scores"], x["num_detections"]]
model = tf.keras.Model(inputs, output) # groepeer lagen tot model
\end{python}

\begin{table}[!ht]
    \caption{Alle operaties die terug te vinden zijn in het TensorFlow Faster-RCNN model en hun compatibiliteit met het ONNX en TFLite framework. De operaties van de ResNet50 backbone zijn in tabel \ref{tab:TFop} terug te vinden.}
\begin{tabular}{ccc}
    \hline
    Operaties & TensorFlow \textrightarrow TFLite & ONNX Opset  \\
    \hline
    BroadcastTo & Ondersteund & 8  \\
    ConcatV2 & Ondersteund & 1  \\
    %Equal & ond & 1 \\
    Exp & Ondersteund & 1 \\
    ExpandDims & Ondersteund & 1 \\
    Fill & Ondersteund & 7 \\
    Floor & Ondersteund & 1 \\
    GatherV2 & Ondersteund & 1  \\
    Greater & Ondersteund & 1  \\
    GreaterEqual & Ondersteund & 1  \\
    Less & Ondersteund & 1 \\
    LogicalAnd & Ondersteund & 1 \\
   % Maximum & const,verw,fus & 1 \\
    Minimum & Ondersteund & 1 \\
    NonMaxSuppressionV5 & Ondersteund & 11 \\
    Range & Ondersteund & 7 \\
    RealDiv & Samengevoegd & 1 \\
    Relu6 & Samengevoegd & 1 \\
    Reshape & Ondersteund & 1 \\
    ResizeBilinear & Ondersteund & 7 \\
   % Round & ond & 11 \\
    SelectV2 & Ondersteund & 7 \\
    Shape & Ondersteund & 1 \\
    Slice & Ondersteund & 1 \\
    Softmax & Ondersteund & 1 \\
    Split & Ondersteund & 1 \\
    %Sqrt & const,verw,fus & 1 \\
    %Square & Verwijderd & 1 \\
    Squeeze & Samengevoegd & 1 \\
    StridedSlice & Ondersteund & 10 \\
    %StatelessIf & / & 1 & / \\
    Sub & Ondersteund & 1 \\
    Sum & Ondersteund & 1 \\
    %Tile & const,verw,fus & 1 \\
    TopKV2 & Ondersteund & 1 \\
    Transpose & Ondersteund & 1 \\
    Unpack & Ondersteund & 1 \\
    Where & Ondersteund & 9 \\
    ZerosLike & Ondersteund & 1 \\
    \hline
\end{tabular}
\label{tab:TF_det_op}
\end{table}

\subsection{Van PyTorch naar PyTorch Mobile}
Het Faster-RCNN model uit de Torchvision bibliotheek kunnen we in Python op de vogende manier inladen.

\begin{python}
model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
\end{python}

Dit model willen we converteren naar een model voor mobiel gebruik zoals we in \ref{py_class} bij het ResNet50 herkenningssysteem gedaan hebben. 
We kunnen echter geen gebruik maken van de \texttt{jit.trace} functie omdat deze geen control flow ondersteunt zoals loops en if/else functies (\ref{trace}).
De \texttt{jit.script} functie heeft deze limitaties niet en kan het Faster-RCNN model succesvol converteren.
Na het converteren kunnen we de scriptmodule verder optimaliseren voor mobiel gebruik.
Bij het opslaan van deze geoptimaliseerde scriptmodule voor mobiel gebruik crasht Google Colaborate zonder een boodschap.
Vermits PyTorch mobile nog in zijn beta fase zit gebeurt er geen correcte foutafhandeling voor niet ondersteunde operaties.
In plaats van het model op te slaan met de \texttt{\_save\_for\_lite\_interpreter} methode zoals bij de ResNet50 herkenner kunnen we het model opslaan als een standaard scriptmodule.
Deze scriptmodule kunnen we echter niet verder optimaliseren voor mobiel gebruik.
We kunnen het Faster-RCNN model op de volgende manier in Python converteren voor mobiel gebruik.

\begin{python}
model.eval() # uitvoering modus
traced_script_module = torch.jit.script(model) # genereer scriptmodule
traced_script_module.to('cpu') # alle data naar cpu runtime
traced_script_module.save('./model.pt') # sla het model op 
\end{python}

Deze scriptmodule kunnen we ook in android implementeren, maar hiervoor moeten we de android\_pytorch bibliotheek importeren in plaats van de android\_pytorch\_lite bibliotheek.
Het gegenereerde model kunnen we vervolgens met Java in Android studio implementeren.

\begin{python}
Module model = Module.load(assetFilePath(MainActivity.this, "model.pt"));
// genereer een input tensor zonder normalisatie
float[] mean = new float[]{0.0f, 0.0f, 0.0f};
float[] std = new float[]{1.0f, 1.0f, 1.0f};
final Tensor input = TensorImageUtils.bitmapToFloat32Tensor(bitmap, mean, std);

// verminder input dimensie van [1,3,416,416] naar [3,416,416]
long shape[] = new long[]{3, 416, 416};
Tensor b = Tensor.fromBlob(input.getDataAsFloatArray(), shape);

// voer het model uit
IValue[] output2 = model.forward(IValue.listFrom(input)).toTuple();
\end{python}

Het TorchVision Faster-RCNN model verwacht geen input die genormaliseerd is volgens de standaard waarden zoals het ResNet50 model \ref{py_class}, waardoor we de mean en std waarden zelf moeten initialiseren.
Bij het uitvoeren van het script model krijgen we een fout dat de .nms opperatie niet wordt ondersteund.
\textcolor{red}{Could not find any similar ops to torchvision::nms. This op may not exist or may not be currently supported in TorchScript.}
\newline
Dit is de non-maxima supression die ervoor zorgt dat de meeste optimale bounding box van een object overblijft. 
PyTorch geeft de mogelijkheid om de torchvision\_ops bibliotheek te implementeren via Gradle.
Bij het toevoegen van onderstaande code aan de \texttt{gradle.build} file in Android studio zouden al de Torchvision operaties ge\"implementeerd moeten zijn.

\begin{python}
implementation 'org.pytorch:pytorch_android:1.8.0'
implementation 'org.pytorch:pytorch_android_torchvision:1.8.0'
implementation 'org.pytorch:torchvision_ops:0.9.0'
\end{python}

Maar om gebruik te maken van deze torchvision\_ops bibliotheek hebben we een model nodig van het Detectron2Go (\cite{Facebook_detectron2_2021}) framework.
Tijdens het importeren van deze bibliotheek krijgen we steeds foutboodschappen dat bepaalde modules meerdere keren aanwezig zijn.
De Torchvisions\_ops bibliotheek importeert bepaalde modules die pytorch\_android ook importeert.

%Maar na het implementeren van deze bibliotheek krijgen we de volgende error ....
We kunnen ook de Torchvision\_ops bibliotheek die terug te vinden is in de Github repository van Torchvision implementeren in het Android studio project.
Dit is een Android studio project dat als module kan worden ingeladen in het PyTorch object detectie project.
Op deze manier kunnen we de Torchvision operaties wel implementeren in Android studio.
Wel moet het PyTorch model volledig onder CPU runtime worden geconverteerd naar een TorchScript model.
Als we niet in CPU runtime converteren krijgen we de volgende fout: \textcolor{red}{com.facebook.jni.CppException: Could not run 'aten::empty\_strided' with arguments from the 'CUDA' backend.}
\newline
Op deze manier kunnen we succesvol een PyTorch Faster-RCNN model uitvoeren op een mobiel toestel.

%PyTorch geeft ons de mogelijkheid om het Faster-RCNN model op te splitsen in verschillende delen.
%We kunnen het model opsplitsen in de volgende delen:

%\begin{lstlisting}
%    model.transforms
%    model.backbone
%    model.rpn.anchor_generator
%    model.rpn.head 
%    model.roi_heads
%\end{lstlisting}

%Dan zien we dat de anchor\_generator het limiterende gedeelte vormt.
%Een andere mogelijke oplossing zou dus zijn om alle delen als aparte netwerken te implementeren en een eigen anchor\_generator functie te schrijven in Android studio.

\subsection{Van PyTorch naar ONNX implementatie}
Zoals bij het TensorFlow Faster-RCNN model is hier ook een minimale opset versie van 11 vereist.
Voor PyTorch is bij een standaard opset de niet ondersteunde operatie de Pad operatie met de volgende error: 
\textcolor{red}{RuntimeError: Unsupported: ONNX export of Pad in opset 9. The sizes of the padding must be constant. Please try opset version 11.}
Al de andere operaties van het Faster-RCNN model worden ondersteund in eerdere opset versies.
De conversie naar ONNX gebeurt op dezelfde manier als de ResNet50 convertie voor PyTorch beschreven in \ref{py_onnx}.
Bij het uitvoeren van het model in Android studio krijgen we een error dat het model te groot is:
\textcolor{red}{java.lang.OutOfMemoryError: Failed to allocate a 247754488 byte allocation with 16831896 free bytes and 16MB until OOM, target footprint 268435456, growth limit 268435456}.
Het uitvoeren van het ONNX model in Android studio lukt dus niet.
Dit probleem kan mogelijk worden opgelost door de quantisatie optimalisatie toe te passen beschreven in \ref{quant}.

\subsection{Samenvatting}
Voor de implementatie van een Faster-RCNN model moeten we rekening houden met enkele zaken.
Bij het TensorFlow model zal de TFLiteConverter de groottes van de input en outputbuffers op 1 zetten.
Om deze reden moeten we eerst de inputgrootte defini\"eren met onderstaande Python code voordat we het model converteren.
Door de cast operatie toe te voegen is het model ook compatibel voor de ONNX implementatie in Android studio.
Want de ONNX API voor Android studio ondersteunt het datatype Uint8 niet.

\begin{python}
layer = hub.KerasLayer(hub_model) 
inputs = tf.keras.Input(shape=[160,160,3], dtype=tf.float32)
x = tf.cast(inputs, dtype=tf.uint8)
x = layer(x) 
output = [x["detection_classes"], x["detection_boxes"], x["detection_scores"], x["num_detections"]]
model = tf.keras.Model(inputs, output) 
\end{python}

Dit model kunnen we converteren naar TFLite zoals het ResNet50 model in \ref{tf_h_conv}.
Via de TensorFlow Lite Interpreter API kunnen we het model uitvoeren in Android studio.
Wel moeten we hier de outputbuffers defini\"eren want de ouput is groter dan het TFLite model verwacht.

Het Pytorch Faster-RCNN model kan met onderstaande code geconverteerd worden naar een model dat uitvoerbaar is op een mobiel toestel.
We kunnen het model niet verder optimaliseren voor mobiel gebruik omdat niet al de operaties ondersteund worden.

\begin{python}
traced_script_module = torch.jit.script(model)
traced_script_module.to('cpu') 
traced_script_module.save('./model.pt') 
\end{python}

Het opgelsagen model kan in Android studio ge\"implementeerd worden.
Omdat niet al de operaties ondersteund worden moeten we eerst een torchvision\_ops module toevoegen aan het Android studio project.
Deze module is terug te vinden in Torchivision Github repository.

Voor ONNX kunnen we enkel het TensorFlow model implementeren in Android studio omdat de bestandsgrootte van het PyTorch model te groot is.
Voor de conversie naar ONNX hebben is een minimum opset versie van 11 nodig.
De NonMaxSuppressionV5 operatie wordt pas vanaf opset versie 11 ondersteund.
Dit model kan eenvoudig geconverteerd worden met het volgende CLI-commando.

\begin{python}
python -m tf2onnx.convert --saved-model ./model --output model.onnx
\end{python}

\section{YOLO naar mobiele implementatie}
Een voorgetraind YOLO model is niet terug te vinden in de TorchVision biblitotheek of TensorFlow object detection API.
We moeten zelf onze detector defini\"eren en vervolgens de voorgetrainde YOLO gewichten inladen.
We kiezen de YOLOV3 architectuur omdat dit de laatste YOLO versie is voorgesteld door \cite{redmon_yolov3_2018}.
Voor de standaard YOLO architectuur met een Darknet backbone kunnen we de gewichten terugvinden op \cite{darknet13} .

\subsection{Van TensorFlow naar TFlite implementatie}
Voor het defini\"eren van het YOLO model en het inladen van de voorgetrainde gewichten gebruiken we een script uit de Github repository van \cite{anh_yolo3_2021}.
Aan de hand van dit script kunnen we op een eenvoudige manier het model in Python inladen.

\begin{python}
!wget https://pjreddie.com/media/files/yolov3.weights
model = make_yolov3_model()
weight_reader = WeightReader('yolov3.weights')
weight_reader.load_weights(model)
\end{python}

Het YOLO model levert al de mogelijke bounding boxes en classificatie voorspellingen.
Waardoor we na het uitvoeren van het model nog de Non-maxima suppresion methode moeten uitvoeren zodat enkel de beste bounding box overblijven per object.

Het model kunnen we eenvoudig converteren naar een TFLite model zoals het ResNet50 model.
Maar zoals bij het Faster-RCNN model wordt het input formaat tijdens het converteren gewijzigd naar [1, 1, 1, 3].
Ook hier moeten we het input formaat specifiek met het model meegeven.
We kunnen er ook voor zorgen dat de output al in het juiste formaat staat zodat we dit niet in Android studio moeten implementeren.

\begin{python}
inputs = tf.keras.Input(shape=[416,416,3], dtype=tf.float32)
output = model(inputs)
output[0] =  tf.reshape(output[0], (1, 13, 13, 3, 85))
output[1] =  tf.reshape(output[1], (1, 26, 26, 3, 85))
output[2] =  tf.reshape(output[2], (1, 52, 52, 3, 85))
model = tf.keras.Model(inputs, output)
\end{python}

Het gegenereerde model kan op dezelfde manier worden uitgevoerd als het Faster-RCNN model in Android studio beschreven in \ref{rcnn_tf}.
Wel moeten we nog de Non-Maxima supression stap zelf implementeren in Android studio.

\subsection{Van TensorFlow naar ONNX implementatie}
De conversie naar ONNX gebeurt op dezelfde manier als ResNet50 en Faster-RCNN.
Er zijn geen limiterende operaties waardoor de standaard opset versie van 9 voldoende is voor de conversie.
Maar omdat het ONNX model een bestandsgrootte heeft 236.28 MB wat groter is dan het PyTorch Faster-RCNN model is het niet implementeerbaar in Android studio.

\subsection{Van PyTorch naar PyTorch mobile implementatie}
Voor het defini\"eren van het Yolo model en het inladen van de voorgetrainde gewichten gebruiken we een script uit de Github repository van \cite{kathuria_pytorch_2022} .
Aan de hand van dit script kunnen we op een eenvoudige manier het model inladen.
De volgende Python code zal het model laden en converteren voor mobiel gebruik.

\begin{python}
model = Darknet("/content/YOLO_v3_tutorial_from_scratch/cfg/yolov3.cfg")
model.load_weights("/content/yolov3.weights")

im = Image.open("/path_naar_afbeelding").resize((416, 416))
convert_tensor = torchvision.transforms.ToTensor()(im) # converteer naar tensor
b = convert_tensor.unsqueeze(0) # voeg een dimensie toe aan input

model.eval()
traced_script_module = torch.jit.trace(model, b)
traced_script_module_optimized = optimize_for_mobile(traced_script_module)
traced_script_module_optimized._save_for_lite_interpreter("./model_s.ptl")
\end{python}

Zoals we in de resultaten zullen zien (\ref{res_size}), zal de \texttt{optimize\_for\_mobile} functie de bestandsgrootte sterk doen dalen.
Dit komt doordat de meeste PyTorch implementaties van YOLO de verschillende lagen in een lijst bewaren.
Op deze manier kunnen we vervolgens de gewichten inlezen en elk element in de lijst itereren zodat we voor elke laag de gewichten kunnen configureren.
Al deze gegevens worden tijdens runtime bewaard.
Bij het uitvoeren van het model zal de forward functie van het model deze lijst itereren en elke laag uitvoeren.
Bij het uitvoeren van de \texttt{jit.trace} functie zal de forward functie van het model opnieuw worden uitgevoerd.
De \texttt{jit.trace} functie houdt enkel rekening met de gegevens in de klasse die het netwerk definieert.
In deze klasse zijn niet de verschillende lagen gedefinieerd maar enkel een lijst die deze lagen bevat.
Hierdoor registreert de trace functie de lijst met de verschillende lagen als een lijst waar telkens een constante waarde wordt uitgehaald.
Als gevolg hiervan zal het torchscript model altijd hetzelfde resultaat produceren.
Om dit op te lossen zouden we het YOLO model zelf kunnen defini\"eren in \'e\'en klasse en vervolgens zelf trainen.
We zouden het probleem ook kunnen oplossen door een methode te ontwikkelen waarbij we de gewichten  zonder de operaties in een lijst te plaatsen kunnen inladen.

\subsection{Van PyTorch naar ONNX implementatie}
Het PyTorch model kunnen we op dezelfde manier converteren als het ResNet50 en Faster-RCNN model zoals beschreven in \ref{py_onnx}.
Hierbij is een opset versie van 11 vereist omdat de operatie index\_put pas ondersteund is vanaf opset versie 11.
Na de conversie is de meegegeven input echter leeg waardoor we onnxruntime niet kunnen uitvoeren omdat we de input niet kunnen meegeven.
De reden hiervoor is dat bij het uitvoeren van onnxruntime er altijd een input met een naam moet worden meegegeven.
Als we het model in Netron (\cite{roeder_lutzroedernetron_2022}) openen zien we dat er geen input is.
Het gegenereerde ONNX model is een gesloten model dat slechts \'e\'en output produceert.
De oorzaak hiervan is hetzelfde als bij het converteren naar PyTorch Mobile.

\subsection{Samenvatting}
De YOLO detector kunnen we enkel via TensorFlow in een mobiele omgeving implementeren.
We configureren het model en laden de YOLO gewichten in via de methode voorgesteld door \cite{anh_yolo3_2021}.
Om ervoor te zorgen dat de TFLiteConverter de inputgrootte niet op 1 zet moet we deze eerst specifi\"eren via onderstaande Python code.
We zorgen er ook voor dat de output al in het juiste formaat staat zodat we deze stappen niet meer moeten implementeren in Android studio.

\begin{python}
inputs = tf.keras.Input(shape=[416,416,3], dtype=tf.float32)
output = model(inputs)
output[0] =  tf.reshape(output[0], (1, 13, 13, 3, 85))
output[1] =  tf.reshape(output[1], (1, 26, 26, 3, 85))
output[2] =  tf.reshape(output[2], (1, 52, 52, 3, 85))
model = tf.keras.Model(inputs, output)
\end{python}

Zoals bij Faster-RCNN kunnen we dit model op de standaard manier converteren naar TFLite.
Vervolgens kunnen we dit model implementeren via de TensorFlow Lite Interpreter API.