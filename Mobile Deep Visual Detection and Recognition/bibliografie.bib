
@book{jiang_deep_2019,
	address = {Singapore},
	edition = {1st ed. 2019.},
	title = {Deep {Learning} in {Object} {Detection} and {Recognition}, edited by {Xiaoyue} {Jiang}, {Abdenour} {Hadid}, {Yanwei} {Pang}, {Eric} {Granger}, {Xiaoyi} {Feng}.},
	isbn = {978-981-10-5152-4},
	abstract = {This book discusses recent advances in object detection and recognition using deep learning methods, which have achieved great success in the field of computer vision and image processing. It provides a systematic and methodical overview of the latest developments in deep learning theory and its applications to computer vision, illustrating them using key topics, including object detection, face analysis, 3D object recognition, and image retrieval. The book offers a rich blend of theory and practice. It is suitable for students, researchers and practitioners interested in deep learning, computer vision and beyond and can also be used as a reference book. The comprehensive comparison of various deep-learning applications helps readers with a basic understanding of machine learning and calculus grasp the theories and inspires applications in other computer vision tasks.},
	language = {eng},
	publisher = {Springer Singapore : Imprint: Springer},
	author = {Jiang, Xiaoyue and Hadid, Abdenour and Pang, Yanwei and Granger, Eric and Feng, Xiaoyi},
	year = {2019},
	keywords = {Data mining., Optical data processing., Pattern recognition.},
}

@misc{koehrsen_neural_2018,
	title = {Neural {Network} {Embeddings} {Explained}},
	url = {https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526},
	abstract = {How deep learning can represent War and Peace as a vector},
	language = {en},
	urldate = {2021-09-21},
	journal = {Medium},
	author = {Koehrsen, Will},
	month = oct,
	year = {2018},
	file = {Snapshot:files/56/neural-network-embeddings-explained-4d028e6f0526.html:text/html},
}

@misc{saha_comprehensive_2018,
	title = {A {Comprehensive} {Guide} to {Convolutional} {Neural} {Networks} — the {ELI5} way},
	url = {https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53},
	abstract = {Artificial Intelligence has been witnessing a monumental growth in bridging the gap between the capabilities of humans and machines…},
	language = {en},
	urldate = {2021-09-21},
	journal = {Medium},
	author = {Saha, Sumit},
	month = dec,
	year = {2018},
	file = {Snapshot:files/58/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53.html:text/html},
}

@article{du_overview_2020,
	title = {Overview of two-stage object detection algorithms},
	volume = {1544},
	issn = {1742-6596},
	url = {https://doi.org/10.1088/1742-6596/1544/1/012033},
	doi = {10.1088/1742-6596/1544/1/012033},
	abstract = {Nowadays, object detection has gradually become a quite popular field. From the traditional methods to the methods used at this stage, object detection technology has made great progress, and is still continuously developing and innovating. This paper reviews two-stage object detection algorithms used at this stage, explaining in detail the working principles of Faster R-CNN, R-FCN, FPN, and Casecade R-CNN and analyzing the similarities and differences between these four two-stage object detection algorithms. Then we used HSRC2016 ship dataset to perform experiments with Faster R-CNN, R-FCN, FPN, and Casecade R-CNN and compared the effectiveness of them with experimental results.},
	language = {en},
	urldate = {2021-09-21},
	journal = {Journal of Physics: Conference Series},
	author = {Du, Lixuan and Zhang, Rongyu and Wang, Xiaotian},
	month = may,
	year = {2020},
	note = {Publisher: IOP Publishing},
	pages = {012033},
	file = {IOP Full Text PDF:files/62/Du e.a. - 2020 - Overview of two-stage object detection algorithms.pdf:application/pdf},
}

@misc{brownlee_gentle_2019,
	title = {A {Gentle} {Introduction} to the {Rectified} {Linear} {Unit} ({ReLU})},
	url = {https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/},
	abstract = {In a neural network, the activation function is responsible for transforming the summed weighted input from the node into the […]},
	language = {en-US},
	urldate = {2021-09-21},
	journal = {Machine Learning Mastery},
	author = {Brownlee, Jason},
	month = jan,
	year = {2019},
}

@article{girshick_rich_2014,
	title = {Rich feature hierarchies for accurate object detection and semantic segmentation},
	url = {http://arxiv.org/abs/1311.2524},
	abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012---achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\textasciitilde}rbg/rcnn.},
	urldate = {2021-09-22},
	journal = {arXiv:1311.2524 [cs]},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	month = oct,
	year = {2014},
	note = {arXiv: 1311.2524},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Extended version of our CVPR 2014 paper; latest update (v5) includes results using deeper networks (see Appendix G. Changelog)},
	file = {arXiv Fulltext PDF:files/67/Girshick e.a. - 2014 - Rich feature hierarchies for accurate object detec.pdf:application/pdf;arXiv.org Snapshot:files/69/1311.html:text/html},
}

@article{uijlings_selective_2013,
	title = {Selective {Search} for {Object} {Recognition}},
	volume = {104},
	copyright = {Springer Science+Business Media New York 2013},
	issn = {09205691},
	url = {http://www.proquest.com/docview/1412093173/abstract/6D683BAB48F94F66PQ/1},
	doi = {http://dx.doi.org.kuleuven.e-bronnen.be/10.1007/s11263-013-0620-5},
	abstract = {This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 \% recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi.unitn.it/{\textasciitilde}uijlings/SelectiveSearch.html).[PUBLICATION ABSTRACT]},
	language = {English},
	number = {2},
	urldate = {2021-09-22},
	journal = {International Journal of Computer Vision},
	author = {Uijlings, J. R. and R and van de Sande, K. E. and A and Gevers, T. and Smeulders, A. W. and M},
	month = sep,
	year = {2013},
	note = {Num Pages: 154-171
Place: New York, Netherlands
Publisher: Springer Nature B.V.},
	keywords = {Computer science, Computers--Computer Systems, Datasets, Image processing systems, Information management, Localization, Search engines, Software, Studies},
	pages = {154--171},
	file = {Full Text PDF:files/68/Uijlings e.a. - 2013 - Selective Search for Object Recognition.pdf:application/pdf},
}

@article{ren_faster_2016,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	url = {http://arxiv.org/abs/1506.01497},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	urldate = {2021-09-22},
	journal = {arXiv:1506.01497 [cs]},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jan,
	year = {2016},
	note = {arXiv: 1506.01497},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Extended tech report},
	file = {arXiv Fulltext PDF:files/76/Ren e.a. - 2016 - Faster R-CNN Towards Real-Time Object Detection w.pdf:application/pdf;arXiv.org Snapshot:files/77/1506.html:text/html},
}

@misc{noauthor_ieee_nodate,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore-ieee-org.kuleuven.e-bronnen.be/stamp/stamp.jsp?tp=&arnumber=7780460},
	urldate = {2021-09-22},
	file = {IEEE Xplore Full-Text PDF\::files/79/stamp.html:text/html},
}

@inproceedings{redmon_you_2016,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	shorttitle = {You {Only} {Look} {Once}},
	doi = {10.1109/CVPR.2016.91},
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	month = jun,
	year = {2016},
	note = {ISSN: 1063-6919},
	keywords = {Computer architecture, Microprocessors, Neural networks, Object detection, Pipelines, Real-time systems, Training},
	pages = {779--788},
	file = {IEEE Xplore Full Text PDF:files/81/Redmon e.a. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf:application/pdf;IEEE Xplore Abstract Record:files/82/7780460.html:text/html},
}

@inproceedings{liu_ssd_2016,
	title = {{SSD}: {Single} {Shot} {MultiBox} {Detector}},
	volume = {9905},
	isbn = {978-3-319-46447-3},
	shorttitle = {{SSD}},
	doi = {10.1007/978-3-319-46448-0_2},
	abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For {\textbackslash}(300 {\textbackslash}times 300{\textbackslash}) input, SSD achieves 74.3 \% mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for {\textbackslash}(512 {\textbackslash}times 512{\textbackslash}) input, SSD achieves 76.9 \% mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https:// github. com/ weiliu89/ caffe/ tree/ ssd.},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander},
	month = oct,
	year = {2016},
	pages = {21--37},
	file = {Full Text PDF:files/89/Liu e.a. - 2016 - SSD Single Shot MultiBox Detector.pdf:application/pdf},
}

@book{leibe_computer_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Computer {Vision} – {ECCV} 2016: 14th {European} {Conference}, {Amsterdam}, {The} {Netherlands}, {October} 11–14, 2016, {Proceedings}, {Part} {I}},
	volume = {9905},
	isbn = {978-3-319-46447-3 978-3-319-46448-0},
	shorttitle = {Computer {Vision} – {ECCV} 2016},
	url = {http://link.springer.com/10.1007/978-3-319-46448-0},
	language = {en},
	urldate = {2021-09-24},
	publisher = {Springer International Publishing},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	doi = {10.1007/978-3-319-46448-0},
	file = {Leibe e.a. - 2016 - Computer Vision – ECCV 2016 14th European Confere.pdf:files/87/Leibe e.a. - 2016 - Computer Vision – ECCV 2016 14th European Confere.pdf:application/pdf},
}

@misc{little_activation_2020,
	title = {Activation {Functions} ({Linear}/{Non}-linear) in {Deep} {Learning} — {ReLU}/{Sigmoid}/{SoftMax}/{Swish}/{Leaky} {ReLu}.},
	url = {https://xzz201920.medium.com/activation-functions-linear-non-linear-in-deep-learning-relu-sigmoid-softmax-swish-leaky-relu-a6333be712ea},
	abstract = {What is the activation function?},
	language = {en},
	urldate = {2021-09-24},
	journal = {Medium},
	author = {Little, Z²},
	month = may,
	year = {2020},
	file = {Snapshot:files/93/activation-functions-linear-non-linear-in-deep-learning-relu-sigmoid-softmax-swish-leaky-relu-a.html:text/html},
}

@article{girshick_fast_2015,
	title = {Fast {R}-{CNN}},
	url = {http://arxiv.org/abs/1504.08083},
	abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
	urldate = {2021-09-24},
	journal = {arXiv:1504.08083 [cs]},
	author = {Girshick, Ross},
	month = sep,
	year = {2015},
	note = {arXiv: 1504.08083},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: To appear in ICCV 2015},
	file = {arXiv Fulltext PDF:files/96/Girshick - 2015 - Fast R-CNN.pdf:application/pdf;arXiv.org Snapshot:files/100/1504.html:text/html},
}

@article{girshick_fast_2015-1,
	title = {Fast {R}-{CNN}},
	url = {http://arxiv.org/abs/1504.08083},
	abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
	urldate = {2021-09-24},
	journal = {arXiv:1504.08083 [cs]},
	author = {Girshick, Ross},
	month = sep,
	year = {2015},
	note = {arXiv: 1504.08083},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: To appear in ICCV 2015},
	file = {arXiv Fulltext PDF:files/99/Girshick - 2015 - Fast R-CNN.pdf:application/pdf;arXiv.org Snapshot:files/101/1504.html:text/html},
}

@article{han_deep_2016,
	title = {Deep {Compression}: {Compressing} {Deep} {Neural} {Networks} with {Pruning}, {Trained} {Quantization} and {Huffman} {Coding}},
	shorttitle = {Deep {Compression}},
	url = {http://arxiv.org/abs/1510.00149},
	abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.},
	urldate = {2021-09-26},
	journal = {arXiv:1510.00149 [cs]},
	author = {Han, Song and Mao, Huizi and Dally, William J.},
	month = feb,
	year = {2016},
	note = {arXiv: 1510.00149},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Published as a conference paper at ICLR 2016 (oral)},
	file = {arXiv Fulltext PDF:files/105/Han e.a. - 2016 - Deep Compression Compressing Deep Neural Networks.pdf:application/pdf;arXiv.org Snapshot:files/106/1510.html:text/html},
}
